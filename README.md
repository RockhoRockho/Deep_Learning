# Deep_Learning_Tensorflow
Tensorflow Deep Learning

## Day 1 (2021-10-27)
- 딥러닝 상수, 변수, 플레이스홀더 학습

## Day 2 (2021-11-01)
- 딥러닝 선형회귀 이론
- 경사하강법 
- 로지스틱회귀 이론

## Day 3 (2021-11-02)
- 딥러닝 개요
- 딥러닝 기초수학학습

## Day 4 (2021-11-03)
- 신경망 데이터 표현(0, 1, 2, 3, 4, 5차원 텐서)
- 신경망 구조 학습
  - 퍼셉트론
  - 뉴련의 수학적 표현
  - 완전 연결 계층 수학적 표현
  - 논리회로(AND, OR, NAND, XOR 게이트)
  - 다층 퍼셉트론
  - 활성화 함수(Step Function, Sigmoid Function, ReLU, tanh, softmax, Identity Fuction)
  - 3층 신경망 구현
  - 다중로지스틱 회귀 

## Day 5 (2021-11-04)
- 모델학습과 손실함수(원-핫인코딩, 평균절대오차, 평균제곱오차, 교차엔트로피오차)
- 경사하강법학습(볼록함수, 비볼록함수, 전역최적값/지역최적값, 학습률, 안장점)

## Day 6 (2021-11-06)
- 신경망학습
  - 단순한 신경망 구현(AND/OR/NAND/XOR Gate)
  - 다중 클래스 분류(MNIST, 2층신경망 구현)

## Day 7 (2021-11-08)
- 군집화, K-근접이웃 이론 학습

## Day 8 (2021-11-09)
- 오차역전파 학습
  - 활성화함수 역전파(시그모이드, ReLU)
  - 행렬 연산에 대한 역전파
  - MNIST 분류 with 역전파

## Day 9 (2021-11-10)
- 최적화 방법(확률적 경사하강법, SGD의 단점, 모멘텀, AdaGrad, RMSProp, Adam)
- 가중치 초기화
- 비선형 함수에서의 가중치 초기화
- 배치정규화
- 과대적합/과서적합
- 규제화
- 드롭아웃
- 하이퍼 파라미터
- MNIST분류

## Day 10 (2021-11-11)

### 합성곱 신경망 CNN 학습
- 완전 연결계층과의 차이
- 합성곱 연산
- 패딩과 스트라이드
- 풀링
- 2차원 이미지에 대한 필터 연산 예시
- 3차원 데이터의 합성곱 연산
- 합성곱 신경망 구현
- 대표적인 CNN 모델(LeNet-5 , AlexNet, VGG-16)
- CNN 학습구현 - Mnist

## Day 11 (2021-11-15)

- 자연어처리 학습
  - CBOW, Skip-Gram Embedding, Word2Vec
  - 순환신경망 (BPTT, Truncated BPTT, Time RNN Layer)
  - LSTM(forget gate, input gate, output gate

## Day 12 (2021-11-16)

- 텐서플로우 학습(객체, 차원&연산, 난수생성, 즉시실행모드(Eager Mode), <->넘파이, 타입변환, 오토그래프, @tf.function, 변수생성, 자동미분(autograd))
- 간단한 신경망 구조(퍼셉트론학습(가중치 업데이트), AND/OR/XOR Gate 학습
- 시각화 사용(XOR Gate)
